{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN) for Atari-Pong\n",
    "---\n",
    "In this notebook, we will implement a DQN agent with OpenAI Gym's Pong-v0 environment.\n",
    "The main challenge is that we train the Deep Reinforcement Learning agent directly using pixels as inputs.\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "import cv2\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Environment and Agent\n",
    "\n",
    "Initialize the environment in the code cell below. The WarpFrame class is a utility class used to translate the original RGB image provided by the OpenAi Gym environment (260, 160, 3) into a grayscale image of dimension (1, 84, 84), as in the original DQN paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SalandMax\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (84, 84, 1)\n",
      "Number of actions:  6\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "\n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env, width=84, height=84, grayscale=True):\n",
    "        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\"\"\"\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.grayscale = grayscale\n",
    "        if self.grayscale:\n",
    "            self.observation_space = spaces.Box(low=0, high=255,\n",
    "                shape=(self.height, self.width, 1), dtype=np.uint8)\n",
    "        else:\n",
    "            self.observation_space = spaces.Box(low=0, high=255,\n",
    "                shape=(self.height, self.width, 3), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, frame):\n",
    "        if self.grayscale:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "        if self.grayscale:\n",
    "            frame = np.expand_dims(frame, -1)\n",
    "            \n",
    "        frame = np.reshape(frame, (1, 84, 84))\n",
    "        return frame\n",
    "\n",
    "env = WarpFrame(env)\n",
    "\n",
    "env.seed(0)\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watch an untrained agent playing Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn_agent import Agent\n",
    "\n",
    "agent = Agent(in_channels=1, action_size=env.action_space.n, seed=0)\n",
    "\n",
    "# watch an untrained agent\n",
    "state = env.reset()\n",
    "for j in range(200):\n",
    "    action = agent.act(state)\n",
    "    env.render()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break \n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent with DQN\n",
    "\n",
    "Run the code cell below to train the agent from scratch.  You are welcome to amend the supplied values of the parameters in the function, to try to see if you can get better performance!\n",
    "\n",
    "Alternatively, you can skip to the next step below (**4. Watch a Smart Agent!**), to load the saved model weights from a pre-trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tAverage Score: -17.40\n",
      "Episode 20\tAverage Score: -16.95\n",
      "Episode 30\tAverage Score: -17.00\n",
      "Episode 40\tAverage Score: -16.95\n",
      "Episode 50\tAverage Score: -16.98\n",
      "Episode 60\tAverage Score: -16.62\n",
      "Episode 70\tAverage Score: -16.47\n",
      "Episode 80\tAverage Score: -16.38\n",
      "Episode 90\tAverage Score: -16.21\n",
      "Episode 100\tAverage Score: -15.78\n",
      "Episode 110\tAverage Score: -15.27\n",
      "Episode 120\tAverage Score: -14.92\n",
      "Episode 130\tAverage Score: -14.38\n",
      "Episode 140\tAverage Score: -13.91\n",
      "Episode 150\tAverage Score: -13.67\n",
      "Episode 160\tAverage Score: -13.42\n",
      "Episode 170\tAverage Score: -12.89\n",
      "Episode 180\tAverage Score: -12.19\n",
      "Episode 190\tAverage Score: -11.64\n",
      "Episode 200\tAverage Score: -11.36\n",
      "Episode 210\tAverage Score: -11.08\n",
      "Episode 220\tAverage Score: -10.73\n",
      "Episode 230\tAverage Score: -10.37\n",
      "Episode 240\tAverage Score: -9.967\n",
      "Episode 250\tAverage Score: -9.32\n",
      "Episode 260\tAverage Score: -9.01\n",
      "Episode 270\tAverage Score: -8.87\n",
      "Episode 280\tAverage Score: -8.85\n",
      "Episode 290\tAverage Score: -8.62\n",
      "Episode 300\tAverage Score: -8.49\n",
      "Episode 310\tAverage Score: -8.26\n",
      "Episode 320\tAverage Score: -8.09\n",
      "Episode 330\tAverage Score: -7.95\n",
      "Episode 340\tAverage Score: -7.87\n",
      "Episode 350\tAverage Score: -7.90\n",
      "Episode 360\tAverage Score: -7.98\n",
      "Episode 370\tAverage Score: -7.76\n",
      "Episode 380\tAverage Score: -7.59\n",
      "Episode 390\tAverage Score: -7.52\n",
      "Episode 400\tAverage Score: -7.41\n",
      "Episode 410\tAverage Score: -7.25\n",
      "Episode 420\tAverage Score: -7.15\n",
      "Episode 430\tAverage Score: -7.16\n",
      "Episode 440\tAverage Score: -7.14\n",
      "Episode 450\tAverage Score: -6.98\n",
      "Episode 460\tAverage Score: -6.53\n",
      "Episode 470\tAverage Score: -6.50\n",
      "Episode 480\tAverage Score: -6.60\n",
      "Episode 490\tAverage Score: -6.70\n",
      "Episode 500\tAverage Score: -6.74\n",
      "Episode 510\tAverage Score: -6.74\n",
      "Episode 520\tAverage Score: -6.67\n",
      "Episode 530\tAverage Score: -6.60\n",
      "Episode 540\tAverage Score: -6.56\n",
      "Episode 550\tAverage Score: -6.55\n",
      "Episode 560\tAverage Score: -6.63\n",
      "Episode 570\tAverage Score: -6.76\n",
      "Episode 580\tAverage Score: -6.86\n",
      "Episode 590\tAverage Score: -6.94\n",
      "Episode 600\tAverage Score: -7.03\n",
      "Episode 610\tAverage Score: -7.13\n",
      "Episode 620\tAverage Score: -7.29\n",
      "Episode 630\tAverage Score: -7.39\n",
      "Episode 640\tAverage Score: -7.42\n",
      "Episode 650\tAverage Score: -7.49\n",
      "Episode 660\tAverage Score: -7.64\n",
      "Episode 670\tAverage Score: -7.58\n",
      "Episode 680\tAverage Score: -7.38\n",
      "Episode 690\tAverage Score: -7.26\n",
      "Episode 700\tAverage Score: -7.16\n",
      "Episode 710\tAverage Score: -7.18\n",
      "Episode 720\tAverage Score: -7.16\n",
      "Episode 730\tAverage Score: -7.07\n",
      "Episode 740\tAverage Score: -7.10\n",
      "Episode 750\tAverage Score: -7.13\n",
      "Episode 760\tAverage Score: -6.92\n",
      "Episode 770\tAverage Score: -6.91\n",
      "Episode 780\tAverage Score: -6.96\n",
      "Episode 790\tAverage Score: -7.11\n",
      "Episode 800\tAverage Score: -7.20\n",
      "Episode 810\tAverage Score: -7.20\n",
      "Episode 820\tAverage Score: -7.00\n",
      "Episode 830\tAverage Score: -6.96\n",
      "Episode 840\tAverage Score: -6.82\n",
      "Episode 850\tAverage Score: -6.71\n",
      "Episode 860\tAverage Score: -6.76\n"
     ]
    }
   ],
   "source": [
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.05, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            reward = np.clip(reward, -1, 1)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = 0.05 #max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 10 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=18.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Watch a Smart Agent!\n",
    "\n",
    "In the next code cell, you will load the trained weights from file to watch a smart agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights from file\n",
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "for i in range(3):\n",
    "    state = env.reset()\n",
    "    for j in range(200):\n",
    "        action = agent.act(state)\n",
    "        env.render()\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break \n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Explore\n",
    "\n",
    "In this exercise, you have implemented a DQN agent and demonstrated how to use it to solve an OpenAI Gym environment.  To continue your learning, you are encouraged to complete any (or all!) of the following tasks:\n",
    "- Amend the various hyperparameters and network architecture to see if you can get your agent to solve the environment faster.  Once you build intuition for the hyperparameters that work well with this environment, try solving a different OpenAI Gym task with discrete actions!\n",
    "- You may like to implement some improvements such as prioritized experience replay, Double DQN, or Dueling DQN! \n",
    "- Write a blog post explaining the intuition behind the DQN algorithm and demonstrating how to use it to solve an RL environment of your choosing.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
